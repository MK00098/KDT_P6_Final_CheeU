#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CheeU VectorDB Module
ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ ë° ê²€ìƒ‰ ê¸°ëŠ¥
"""

import os
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
from dotenv import load_dotenv

# LangChain ì»´í¬ë„ŒíŠ¸
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.schema import Document

# ChromaDB
import chromadb
from chromadb.config import Settings

# .env íŒŒì¼ ìë™ ë¡œë“œ
load_dotenv()


class CheeUVectorDB:
    """
    CheeU VectorDB ê´€ë¦¬ í´ë˜ìŠ¤
    - ChromaDB ê¸°ë°˜ ë²¡í„° ì €ì¥ì†Œ
    - í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
    - ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ê²€ìƒ‰
    """
    
    def __init__(self, vector_db_path: str = "../papers/vectordb"):
        """
        VectorDB ì´ˆê¸°í™”
        
        Args:
            vector_db_path: ChromaDB ì €ì¥ ê²½ë¡œ
        """
        self.vector_db_path = vector_db_path
        self.logger = logging.getLogger(__name__)
        
        # í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ì„¤ì • (384ì°¨ì›)
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # ChromaDB ì„¤ì •
        self.client = chromadb.PersistentClient(
            path=vector_db_path,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        
        # Chroma VectorStore ì´ˆê¸°í™”
        self.vector_store = None
        self._init_vector_store()
    
    def _init_vector_store(self):
        """ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”"""
        try:
            self.vector_store = Chroma(
                client=self.client,
                embedding_function=self.embeddings,
                collection_name="therapy_content"
            )
            self.logger.info(f"âœ… VectorDB ë¡œë“œ ì™„ë£Œ: {self.vector_db_path}")
        except Exception as e:
            self.logger.error(f"âŒ VectorDB ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def search_basic(self, query: str, k: int = 3) -> List[Document]:
        """
        ê¸°ë³¸ ìœ ì‚¬ë„ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
            
        Returns:
            ê´€ë ¨ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            docs = self.vector_store.similarity_search(query, k=k)
            self.logger.info(f"ğŸ” ê¸°ë³¸ ê²€ìƒ‰ ì™„ë£Œ: {len(docs)}ê°œ ë¬¸ì„œ")
            return docs
        except Exception as e:
            self.logger.error(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def search_with_score(self, query: str, k: int = 3) -> List[tuple]:
        """
        ì ìˆ˜ í¬í•¨ ìœ ì‚¬ë„ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
            
        Returns:
            (ë¬¸ì„œ, ì ìˆ˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            docs_with_scores = self.vector_store.similarity_search_with_score(query, k=k)
            self.logger.info(f"ğŸ” ì ìˆ˜ í¬í•¨ ê²€ìƒ‰ ì™„ë£Œ: {len(docs_with_scores)}ê°œ ë¬¸ì„œ")
            return docs_with_scores
        except Exception as e:
            self.logger.error(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def search_with_priority_weighting(self, 
                                     main_query: str,
                                     sub_queries: List[str],
                                     main_weight: float = 0.7,
                                     sub_weight: float = 0.3,
                                     k: int = 3) -> List[Document]:
        """
        ìš°ì„ ìˆœìœ„ ê°€ì¤‘ì¹˜ ê¸°ë°˜ ê²€ìƒ‰
        
        Args:
            main_query: ë©”ì¸ ê²€ìƒ‰ ì¿¼ë¦¬ (ì‚¬ìš©ì ì…ë ¥ + ìŠ¤íŠ¸ë ˆìŠ¤ ìœ í˜•)
            sub_queries: ì„œë¸Œ ê²€ìƒ‰ ì¿¼ë¦¬ë“¤ (ì—°ë ¹, ì„±ë³„, ì§êµ°, í‚¤ì›Œë“œ)
            main_weight: ë©”ì¸ ì¿¼ë¦¬ ê°€ì¤‘ì¹˜
            sub_weight: ì„œë¸Œ ì¿¼ë¦¬ ê°€ì¤‘ì¹˜
            k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
            
        Returns:
            ìš°ì„ ìˆœìœ„ ì¡°ì •ëœ ê´€ë ¨ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # 1. ë©”ì¸ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
            main_docs_with_scores = self.vector_store.similarity_search_with_score(
                main_query, k=k*2
            )
            
            # 2. ì„œë¸Œ ì¿¼ë¦¬ë“¤ë¡œ ê²€ìƒ‰
            sub_docs_with_scores = []
            for sub_query in sub_queries:
                if sub_query.strip():
                    sub_results = self.vector_store.similarity_search_with_score(
                        sub_query, k=k
                    )
                    sub_docs_with_scores.extend(sub_results)
            
            # 3. ê°€ì¤‘ì¹˜ ì ìš© ë° í†µí•©
            doc_scores = {}
            
            # ë©”ì¸ ì¿¼ë¦¬ ì ìˆ˜ ì ìš©
            for doc, score in main_docs_with_scores:
                doc_id = doc.page_content[:100]  # ë¬¸ì„œ ì‹ë³„ìš©
                doc_scores[doc_id] = {
                    'doc': doc,
                    'score': score * main_weight,
                    'main_score': score
                }
            
            # ì„œë¸Œ ì¿¼ë¦¬ ì ìˆ˜ ì ìš©
            for doc, score in sub_docs_with_scores:
                doc_id = doc.page_content[:100]
                if doc_id in doc_scores:
                    doc_scores[doc_id]['score'] += score * (sub_weight / len(sub_queries))
                else:
                    doc_scores[doc_id] = {
                        'doc': doc,
                        'score': score * (sub_weight / len(sub_queries)),
                        'main_score': 1.0
                    }
            
            # 4. ì ìˆ˜ìˆœ ì •ë ¬ ë° ìƒìœ„ kê°œ ë°˜í™˜ (ë†’ì€ ì ìˆ˜ë¶€í„°)
            sorted_docs = sorted(
                doc_scores.values(), 
                key=lambda x: x['score'],
                reverse=True
            )[:k]
            
            result_docs = [item['doc'] for item in sorted_docs]
            
            self.logger.info(f"ğŸ¯ ìš°ì„ ìˆœìœ„ ê²€ìƒ‰ ì™„ë£Œ: {len(result_docs)}ê°œ ë¬¸ì„œ")
            return result_docs
            
        except Exception as e:
            self.logger.error(f"âŒ ìš°ì„ ìˆœìœ„ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            # í´ë°±: ê¸°ë³¸ ê²€ìƒ‰
            return self.search_basic(main_query, k)
    
    def calculate_search_confidence(self, docs: List[Document], 
                                  keywords: List[str]) -> float:
        """
        ê²€ìƒ‰ ê²°ê³¼ ì‹ ë¢°ë„ ê³„ì‚°
        
        Args:
            docs: ê²€ìƒ‰ëœ ë¬¸ì„œë“¤
            keywords: ì‚¬ìš©ì í‚¤ì›Œë“œë“¤
            
        Returns:
            ì‹ ë¢°ë„ ì ìˆ˜ (0.0-1.0)
        """
        if not docs:
            return 0.0
        
        total_score = 0.0
        keyword_matches = 0
        
        for doc in docs:
            content = doc.page_content.lower()
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
            for keyword in keywords:
                if keyword.lower() in content:
                    keyword_matches += 1
            
            # ë¬¸ì„œ ê¸¸ì´ ì ìˆ˜ (ì ì ˆí•œ ê¸¸ì´)
            content_length = len(content)
            if 100 <= content_length <= 1000:
                total_score += 0.3
            elif content_length > 50:
                total_score += 0.1
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ë¹„ìœ¨
        if keywords:
            keyword_ratio = keyword_matches / (len(keywords) * len(docs))
            total_score += keyword_ratio * 0.7
        
        # ë¬¸ì„œ ìˆ˜ ë³´ì •
        doc_count_score = min(len(docs) / 3.0, 1.0) * 0.2
        total_score += doc_count_score
        
        return min(total_score, 1.0)
    
    def get_collection_info(self) -> Dict[str, Any]:
        """
        ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ
        
        Returns:
            ì»¬ë ‰ì…˜ ë©”íƒ€ë°ì´í„°
        """
        try:
            collection = self.client.get_collection("therapy_content")
            count = collection.count()
            
            return {
                "collection_name": "therapy_content",
                "document_count": count,
                "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
                "db_path": self.vector_db_path,
                "status": "active"
            }
        except Exception as e:
            self.logger.error(f"âŒ ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                "status": "error",
                "error": str(e)
            }
    
    def health_check(self) -> bool:
        """
        VectorDB ìƒíƒœ í™•ì¸
        
        Returns:
            ì •ìƒ ìƒíƒœ ì—¬ë¶€
        """
        try:
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
            test_docs = self.search_basic("ìŠ¤íŠ¸ë ˆìŠ¤", k=1)
            return len(test_docs) > 0
        except Exception as e:
            self.logger.error(f"âŒ VectorDB ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            return False


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    logging.basicConfig(level=logging.INFO)
    
    vectordb = CheeUVectorDB()
    
    # ìƒíƒœ í™•ì¸
    print("ğŸ” VectorDB ìƒíƒœ í™•ì¸:")
    print(f"ì •ìƒ ìƒíƒœ: {vectordb.health_check()}")
    
    # ì»¬ë ‰ì…˜ ì •ë³´
    print("\nğŸ“Š ì»¬ë ‰ì…˜ ì •ë³´:")
    info = vectordb.get_collection_info()
    for key, value in info.items():
        print(f"  {key}: {value}")
    
    # ê¸°ë³¸ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\nğŸ” ê¸°ë³¸ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸:")
    docs = vectordb.search_basic("ìš°ìš¸ ìŠ¤íŠ¸ë ˆìŠ¤", k=2)
    for i, doc in enumerate(docs):
        print(f"  [{i+1}] {doc.page_content[:100]}...")
    
    # ìš°ì„ ìˆœìœ„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\nğŸ¯ ìš°ì„ ìˆœìœ„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸:")
    priority_docs = vectordb.search_with_priority_weighting(
        main_query="ìš°ìš¸ ë¬´ê¸°ë ¥",
        sub_queries=["20ëŒ€", "ì—¬ì„±", "ì˜ë£Œì§„", "í”¼ë¡œ"],
        k=2
    )
    for i, doc in enumerate(priority_docs):
        print(f"  [{i+1}] {doc.page_content[:100]}...")