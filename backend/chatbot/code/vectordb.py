#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CheeU Vector DB Module - Production Optimized
ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ ë° ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ê²€ìƒ‰ ê¸°ëŠ¥
"""

import logging
from typing import List, Dict, Any
from pathlib import Path

# LangChain ì»´í¬ë„ŒíŠ¸
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.schema import Document

# ChromaDB
import chromadb
from chromadb.config import Settings


class CheeUVectorDB:
    """
    CheeU VectorDB ê´€ë¦¬ í´ë˜ìŠ¤ - í”„ë¡œë•ì…˜ ìµœì í™”
    - ChromaDB ê¸°ë°˜ ë²¡í„° ì €ì¥ì†Œ
    - í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
    - ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ê²€ìƒ‰ (70/30 ê°€ì¤‘ì¹˜)
    - ë°°ì¹˜ ì²˜ë¦¬ ë° ìºì‹± ì§€ì›
    """
    
    def __init__(self,
                 vector_db_path: str = None,
                 embedding_model: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                 collection_name: str = "therapy_content",
                 device: str = "cpu"):
        """
        VectorDB ì´ˆê¸°í™”
        
        Args:
            vector_db_path: ChromaDB ì €ì¥ ê²½ë¡œ (Noneì‹œ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©)
            embedding_model: ì„ë² ë”© ëª¨ë¸ëª…
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
            device: ë””ë°”ì´ìŠ¤ ('cpu' ë˜ëŠ” 'cuda')
        """
        self.logger = logging.getLogger(__name__)
        
        # ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
        if vector_db_path is None:
            base_dir = Path(__file__).parent.parent
            vector_db_path = str(base_dir / "data" / "vectordb")
        
        self.vector_db_path = vector_db_path
        self.collection_name = collection_name
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        Path(vector_db_path).mkdir(parents=True, exist_ok=True)
        
        # í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ì„¤ì •
        self.embeddings = HuggingFaceEmbeddings(
            model_name=embedding_model,
            model_kwargs={'device': device},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # ChromaDB ì„¤ì • (í”„ë¡œë•ì…˜ ìµœì í™”)
        self.client = chromadb.PersistentClient(
            path=vector_db_path,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=False,  # í”„ë¡œë•ì…˜ì—ì„œëŠ” ë¦¬ì…‹ ë¹„í™œì„±í™”
                is_persistent=True
            )
        )
        
        # ì„±ëŠ¥ ìºì‹œ ì´ˆê¸°í™”
        self._search_cache = {}
        self._cache_max_size = 100
        
        # Chroma VectorStore ì´ˆê¸°í™”
        self.vector_store = None
        self._init_vector_store()
    
    def _init_vector_store(self):
        """ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” (ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”)"""
        try:
            self.vector_store = Chroma(
                client=self.client,
                embedding_function=self.embeddings,
                collection_name=self.collection_name
            )
            
            # ì»¬ë ‰ì…˜ ì •ë³´ ë¡œê¹…
            collection_info = self.get_collection_info()
            doc_count = collection_info.get("document_count", 0)
            
            if doc_count > 0:
                self.logger.info(f"âœ… VectorDB ë¡œë“œ ì™„ë£Œ: {self.vector_db_path} ({doc_count}ê°œ ë¬¸ì„œ)")
            else:
                self.logger.warning(f"âš ï¸ VectorDB ë¹ˆ ìƒíƒœ: {self.vector_db_path}")
                
        except Exception as e:
            self.logger.error(f"âŒ VectorDB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def search_basic(self, query: str, k: int = 3) -> List[Document]:
        """
        ê¸°ë³¸ ìœ ì‚¬ë„ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
            
        Returns:
            ê´€ë ¨ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ìºì‹œ í™•ì¸
            cache_key = f"basic_{query}_{k}"
            if cache_key in self._search_cache:
                self.logger.debug(f"ğŸ¯ ìºì‹œ íˆíŠ¸: {query[:30]}...")
                return self._search_cache[cache_key]
            
            docs = self.vector_store.similarity_search(query, k=k)
            
            # ìºì‹œ ì €ì¥ (í¬ê¸° ì œí•œ)
            if len(self._search_cache) < self._cache_max_size:
                self._search_cache[cache_key] = docs
            
            self.logger.info(f"ğŸ” ê¸°ë³¸ ê²€ìƒ‰ ì™„ë£Œ: {len(docs)}ê°œ ë¬¸ì„œ")
            return docs
            
        except Exception as e:
            self.logger.error(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def search_with_score(self, query: str, k: int = 3) -> List[tuple]:
        """
        ì ìˆ˜ í¬í•¨ ìœ ì‚¬ë„ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
            
        Returns:
            (ë¬¸ì„œ, ì ìˆ˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            docs_with_scores = self.vector_store.similarity_search_with_score(query, k=k)
            self.logger.info(f"ğŸ” ì ìˆ˜ í¬í•¨ ê²€ìƒ‰ ì™„ë£Œ: {len(docs_with_scores)}ê°œ ë¬¸ì„œ")
            return docs_with_scores
            
        except Exception as e:
            self.logger.error(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def search_with_priority_weighting(self, 
                                     main_query: str,
                                     sub_queries: List[str],
                                     main_weight: float = 0.7,
                                     sub_weight: float = 0.3,
                                     k: int = 3) -> List[Document]:
        """
        ìš°ì„ ìˆœìœ„ ê°€ì¤‘ì¹˜ ê¸°ë°˜ ê²€ìƒ‰ (í•µì‹¬ ì•Œê³ ë¦¬ì¦˜)
        
        Args:
            main_query: ë©”ì¸ ê²€ìƒ‰ ì¿¼ë¦¬ (ì‚¬ìš©ì ì…ë ¥ + ìŠ¤íŠ¸ë ˆìŠ¤ ìœ í˜•)
            sub_queries: ì„œë¸Œ ê²€ìƒ‰ ì¿¼ë¦¬ë“¤ (ì—°ë ¹, ì„±ë³„, ì§êµ°, í‚¤ì›Œë“œ)
            main_weight: ë©”ì¸ ì¿¼ë¦¬ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’: 0.7)
            sub_weight: ì„œë¸Œ ì¿¼ë¦¬ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’: 0.3)
            k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
            
        Returns:
            ìš°ì„ ìˆœìœ„ ì¡°ì •ëœ ê´€ë ¨ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ìºì‹œ í™•ì¸
            cache_key = f"priority_{main_query}_{hash(str(sub_queries))}_{k}"
            if cache_key in self._search_cache:
                self.logger.debug(f"ğŸ¯ ìš°ì„ ìˆœìœ„ ìºì‹œ íˆíŠ¸: {main_query[:30]}...")
                return self._search_cache[cache_key]
            
            # 1. ë©”ì¸ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ (ë” ë§ì€ í›„ë³´ í™•ë³´)
            main_docs_with_scores = self.vector_store.similarity_search_with_score(
                main_query, k=k*3  # 3ë°° ë§ì€ í›„ë³´
            )
            
            # 2. ì„œë¸Œ ì¿¼ë¦¬ë“¤ë¡œ ê²€ìƒ‰
            sub_docs_with_scores = []
            for sub_query in sub_queries:
                if sub_query and sub_query.strip():
                    try:
                        sub_results = self.vector_store.similarity_search_with_score(
                            sub_query, k=k*2  # 2ë°° í›„ë³´
                        )
                        sub_docs_with_scores.extend(sub_results)
                    except Exception as e:
                        self.logger.warning(f"ì„œë¸Œ ì¿¼ë¦¬ ê²€ìƒ‰ ì‹¤íŒ¨ '{sub_query}': {e}")
                        continue
            
            # 3. ê°€ì¤‘ì¹˜ ì ìš© ë° í†µí•©
            doc_scores = {}
            
            # ë©”ì¸ ì¿¼ë¦¬ ì ìˆ˜ ì ìš© (ê±°ë¦¬ -> ìœ ì‚¬ë„ ë³€í™˜: 1 - distance)
            for doc, distance in main_docs_with_scores:
                doc_id = doc.page_content[:100]  # ë¬¸ì„œ ì‹ë³„ìš©
                similarity = max(0, 1 - distance)  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
                doc_scores[doc_id] = {
                    'doc': doc,
                    'score': similarity * main_weight,
                    'main_score': similarity,
                    'sub_score': 0.0
                }
            
            # ì„œë¸Œ ì¿¼ë¦¬ ì ìˆ˜ ì ìš©
            sub_weight_per_query = sub_weight / max(len([q for q in sub_queries if q.strip()]), 1)
            for doc, distance in sub_docs_with_scores:
                doc_id = doc.page_content[:100]
                similarity = max(0, 1 - distance)
                
                if doc_id in doc_scores:
                    doc_scores[doc_id]['score'] += similarity * sub_weight_per_query
                    doc_scores[doc_id]['sub_score'] += similarity * sub_weight_per_query
                else:
                    doc_scores[doc_id] = {
                        'doc': doc,
                        'score': similarity * sub_weight_per_query,
                        'main_score': 0.0,
                        'sub_score': similarity * sub_weight_per_query
                    }
            
            # 4. ì ìˆ˜ìˆœ ì •ë ¬ ë° ìƒìœ„ kê°œ ë°˜í™˜
            sorted_docs = sorted(
                doc_scores.values(), 
                key=lambda x: x['score'],
                reverse=True
            )[:k]
            
            result_docs = [item['doc'] for item in sorted_docs]
            
            # ìºì‹œ ì €ì¥
            if len(self._search_cache) < self._cache_max_size:
                self._search_cache[cache_key] = result_docs
            
            # ì„±ëŠ¥ ë¡œê¹…
            avg_score = sum(item['score'] for item in sorted_docs) / len(sorted_docs) if sorted_docs else 0
            self.logger.info(f"ğŸ¯ ìš°ì„ ìˆœìœ„ ê²€ìƒ‰ ì™„ë£Œ: {len(result_docs)}ê°œ ë¬¸ì„œ (í‰ê· ì ìˆ˜: {avg_score:.3f})")
            
            return result_docs
            
        except Exception as e:
            self.logger.error(f"âŒ ìš°ì„ ìˆœìœ„ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            # í´ë°±: ê¸°ë³¸ ê²€ìƒ‰
            return self.search_basic(main_query, k)
    
    def calculate_search_confidence(self, docs: List[Document], 
                                  keywords: List[str]) -> float:
        """
        ê²€ìƒ‰ ê²°ê³¼ ì‹ ë¢°ë„ ê³„ì‚° (ê°œì„ ëœ ì•Œê³ ë¦¬ì¦˜)
        
        Args:
            docs: ê²€ìƒ‰ëœ ë¬¸ì„œë“¤
            keywords: ì‚¬ìš©ì í‚¤ì›Œë“œë“¤
            
        Returns:
            ì‹ ë¢°ë„ ì ìˆ˜ (0.0-1.0)
        """
        if not docs:
            return 0.0
        
        total_score = 0.0
        keyword_matches = 0
        total_keywords = len(keywords) if keywords else 1
        
        for doc in docs:
            content = doc.page_content.lower()
            doc_score = 0.0
            
            # 1. í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ (ê°€ì¤‘ì¹˜: 50%)
            doc_keyword_matches = 0
            for keyword in keywords:
                if keyword and keyword.lower() in content:
                    doc_keyword_matches += 1
            
            keyword_ratio = doc_keyword_matches / total_keywords
            doc_score += keyword_ratio * 0.5
            keyword_matches += doc_keyword_matches
            
            # 2. ë¬¸ì„œ ê¸¸ì´ ì ìˆ˜ (ê°€ì¤‘ì¹˜: 20%)
            content_length = len(content)
            if 100 <= content_length <= 1000:
                doc_score += 0.2  # ì ì ˆí•œ ê¸¸ì´
            elif 50 <= content_length <= 1500:
                doc_score += 0.1  # í—ˆìš© ê°€ëŠ¥í•œ ê¸¸ì´
            
            # 3. ë©”íƒ€ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ (ê°€ì¤‘ì¹˜: 15%)
            metadata = doc.metadata
            if metadata.get('filename'):
                doc_score += 0.1
            if metadata.get('source') == 'research_paper':
                doc_score += 0.05
            
            # 4. ì»¨í…ìŠ¤íŠ¸ í’ë¶€ë„ ì ìˆ˜ (ê°€ì¤‘ì¹˜: 15%)
            sentences = content.count('.')
            if sentences >= 3:
                doc_score += 0.15
            elif sentences >= 1:
                doc_score += 0.08
            
            total_score += doc_score
        
        # ì •ê·œí™” ë° ë¬¸ì„œ ìˆ˜ ë³´ì •
        avg_score = total_score / len(docs)
        doc_count_bonus = min(len(docs) / 3.0, 1.0) * 0.1
        
        final_confidence = min(avg_score + doc_count_bonus, 1.0)
        
        self.logger.debug(f"ğŸ“Š ì‹ ë¢°ë„ ê³„ì‚°: {final_confidence:.3f} ({len(docs)}ê°œ ë¬¸ì„œ, {keyword_matches}/{total_keywords*len(docs)} í‚¤ì›Œë“œ)")
        
        return final_confidence
    
    def add_documents(self, documents: List[Document], batch_size: int = 50):
        """
        ë¬¸ì„œ ë°°ì¹˜ ì¶”ê°€ (í”„ë¡œë•ì…˜ ìµœì í™”)
        
        Args:
            documents: ì¶”ê°€í•  ë¬¸ì„œë“¤
            batch_size: ë°°ì¹˜ í¬ê¸°
        """
        try:
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            for i in range(0, len(documents), batch_size):
                batch = documents[i:i+batch_size]
                self.vector_store.add_documents(batch)
                self.logger.info(f"ğŸ“¥ ë¬¸ì„œ ë°°ì¹˜ ì¶”ê°€: {i+1}-{min(i+batch_size, len(documents))}/{len(documents)}")
            
            # ìºì‹œ í´ë¦¬ì–´
            self._search_cache.clear()
            self.logger.info(f"âœ… ì´ {len(documents)}ê°œ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            raise
    
    def get_collection_info(self) -> Dict[str, Any]:
        """
        ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ (ìƒì„¸ ì •ë³´ í¬í•¨)
        
        Returns:
            ì»¬ë ‰ì…˜ ë©”íƒ€ë°ì´í„°
        """
        try:
            collection = self.client.get_collection(self.collection_name)
            count = collection.count()
            
            # ì¶”ê°€ í†µê³„ ê³„ì‚°
            if count > 0:
                # ìƒ˜í”Œ ë¬¸ì„œ ì¡°íšŒ
                sample_docs = self.search_basic("í…ŒìŠ¤íŠ¸", k=1)
                avg_doc_length = len(sample_docs[0].page_content) if sample_docs else 0
            else:
                avg_doc_length = 0
            
            return {
                "collection_name": self.collection_name,
                "document_count": count,
                "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
                "embedding_dimension": 384,
                "db_path": self.vector_db_path,
                "avg_document_length": avg_doc_length,
                "cache_size": len(self._search_cache),
                "status": "active",
                "created_at": "2024-01-01T00:00:00"
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                "status": "error",
                "error": str(e),
                "document_count": 0
            }
    
    def health_check(self) -> bool:
        """
        VectorDB ìƒíƒœ í™•ì¸ (ê°•í™”ëœ í—¬ìŠ¤ì²´í¬)
        
        Returns:
            ì •ìƒ ìƒíƒœ ì—¬ë¶€
        """
        try:
            # 1. ì»¬ë ‰ì…˜ ì ‘ê·¼ í™•ì¸
            info = self.get_collection_info()
            if info.get("status") == "error":
                return False
            
            # 2. ê²€ìƒ‰ ê¸°ëŠ¥ í™•ì¸
            test_docs = self.search_basic("í…ŒìŠ¤íŠ¸", k=1)
            
            # 3. ë¬¸ì„œ ìˆ˜ í™•ì¸
            doc_count = info.get("document_count", 0)
            if doc_count == 0:
                self.logger.warning("âš ï¸ VectorDBì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            self.logger.info(f"âœ… VectorDB ì •ìƒ ìƒíƒœ: {doc_count}ê°œ ë¬¸ì„œ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ VectorDB ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            return False
    
    def clear_cache(self):
        """ê²€ìƒ‰ ìºì‹œ í´ë¦¬ì–´"""
        self._search_cache.clear()
        self.logger.info("ğŸ§¹ ê²€ìƒ‰ ìºì‹œ í´ë¦¬ì–´ ì™„ë£Œ")